Task Prompt â€¢ Build a Dockerâ€‘First â€œAwesomeâ€‘Listâ€¯Researcherâ€  (Full Aggregate SpecÂ v2)

MissionÂ Â â€”Â Create a productionâ€‘ready tool that ingests the GitHub URL of any Awesomeâ€‘style repository, derives its raw README.md, parses it to JSON, and launches a multiâ€‘agent OpenAI workflow to discover new, nonâ€‘duplicate, specâ€‘compliant resources.  All artifacts are merged into Markdown that passes awesomeâ€‘lint.  Everything runs inside Docker via ./build-and-run.sh; no host Python or external CI.  The system enforces wallâ€‘time and cost ceilings, logs every action, and offers configurable OpenAI model selection for each agent role.

â¸»

1Â â€¢ Reference Knowledge

1.1Â Awesomeâ€‘List Spec (key rules)
	â€¢	TitleÂ + tagline; optional badge row.
	â€¢	Only ## and ### headings; items * [Name](URL) â€“ description â‰¤â€¯100Â chars.
	â€¢	Alphabetical, HTTPSÂ +Â 200â€¯OK, mandatory Contributing.
	â€¢	awesomeâ€‘lint must pass.

1.2Â OpenAI Agents SDK Researchâ€‘Bot Insights
	â€¢	ResearchBot, SearchTool, BrowserTool, Parallel([...]) for concurrency.
	â€¢	Cost guard via usage.total_cost_usd.
	â€¢	Callback hooks on_tool_start/finish for granular logs.

1.3Â Raw Markdown Derivation
example - https://raw.githubusercontent.com/krzemienski/awesome-video/refs/heads/master/README.md
	2.	Build: https://raw.githubusercontent.com/{owner}/{reponame}/refs/heads/{main or master}/README.md
	3.	Fallback /HEAD/README.md if API fails.

â¸»

2Â â€¢ Nonâ€‘Negotiable Constraints

#	Rule
1	Dockerâ€‘only â€“ Image from Dockerfile (pythonâ€¯3.12â€‘slimÂ + PoetryÂ + awesomeâ€‘lintÂ + openaiâ€‘agents).
2	Live operations â€“ No mocks or placeholders.
3	Structured logging â€“ ISOÂ 8601 timestamps, event, tokens, cost USD.
4	CLI flags / env vars
Â Â 	--repo_url (required)
Â Â 	--wall_time (s, defaultÂ 600)
Â Â 	--cost_ceiling (USD, defaultÂ 5.00)
Â Â 	--min_stars (GitHub, defaultÂ 100)
Â Â 	--output_dir (defaultÂ runs/)
Â Â 	--seed (int; omitÂ â†’ random)
Â Â 	--model_planner (default gptâ€‘4.1â€‘mini)
Â Â 	--model_researcher (default o3)
Â Â 	--model_validator (default o3)
Â Â 	EnvÂ OPENAI_API_KEYÂ (required)
5	Outputs under runs/<ISOâ€‘TS>/ (see Â§7.2).
6	Functional test â€“ tests/run_e2e.sh; no pytest/CI.
7	PEPÂ 8 & wholeâ€‘file codegen.
8	Dedup guarantee â€“ new_links.json âˆ© original.json = âˆ….
9	Rateâ€‘limit resilience â€“ retry/backâ€‘off.
10	Cost ceiling only â€“ No token caps; terminate when predicted spend â‰¥Â ceiling.


â¸»

3Â â€¢ Modelâ€‘Selection Strategy

Agent Role	Default Model	Rationale	Override Flag
PlannerAgent	gptâ€‘4.1â€‘mini	Requires deeper reasoning to craft highâ€‘quality, diverse search queries; accuracy prioritized.	--model_planner
CategoryResearchAgent (parallel)	o3	Large volume of lightweight webâ€‘search prompts; lower latency & cost are ideal.	--model_researcher
Validator (optional LLM description cleanup)	o3	Simple transformations (description trimming); inexpensive.	--model_validator

If a flag is provided, all instances of that agent will use the specified model parameter in openai.chat.completions.

â¸»

4Â â€¢ Cost Guard Logic

est_cost = price_per_1k_tokens(model) * expected_tokens / 1000
if usage.total_cost_usd + est_cost > cost_ceiling:
    logger.warning("Cost ceiling reached â€“ halting further calls.")
    break


â¸»

5Â â€¢ System Architecture

flowchart TD
    A[--repo_url] --> B[Derive RAW README]
    B --> C[awesome_parser.py â†’ original.json]
    C --> D[PlannerAgent (model_planner)]
    D --> E[plan.json]
    E --> F[CategoryResearchAgentsÃ—N (model_researcher)]
    F --> G[Aggregator]
    G --> H[DuplicateFilter]
    H --> I[Validator (model_validator)]
    I --> J[new_links.json]
    J & C --> K[renderer.py â†’ updated_list.md]
    K --> L[awesome-lint âœ“] --> M[Persist artifacts]


â¸»

6Â â€¢ Module Details
	â€¢	awesome_parser.pyÂ â€“ parse Markdown âœ JSON; Bloom filter.
	â€¢	planner_agent.pyÂ â€“ uses selected model; synonym expansion + random shuffle (seeded).
	â€¢	category_agent.pyÂ â€“ parallel; leverages SearchTool/BrowserTool.
	â€¢	aggregator.py / duplicate_filter.pyÂ â€“ merge + dedup ratio guard.
	â€¢	validator.pyÂ â€“ HTTPÂ HEAD, GitHub stars, SPDX license, description cleanup via chosen model.
	â€¢	renderer.pyÂ â€“ merge, lintâ€‘fix loop.
	â€¢	main.pyÂ â€“ orchestrator; wallâ€‘time via signal.alarm.

â¸»

7Â â€¢ Cursor Workflow (MCP)
	â€¢	Load ContextÂ 7 via MCP at start of each automated task; utilise MCP utilities (FileGraph, DependencyGraph).
	â€¢	Branch per feature; squash merge; tag next steps with cursor.task:.
	â€¢	All commands executed via container entrypoint.

â¸»

8Â â€¢ Deliverables

8.1Â Repository Assets
	â€¢	Dockerfile, build-and-run.sh, core modules, README.md (user guideÂ + model flag docs), architecture.md, CONTRIBUTING_TEMPLATE.md, tests/run_e2e.sh, .gitignore.

8.2Â Runtime Artifacts
	â€¢	original.json, plan.json, candidate_*.json, new_links.json, updated_list.md, agent.log, research_report.md.
	â€¢	Example run directory under examples/sample_run/ (uncompressed).

8.3Â Quality Gates
	â€¢	awesome-lint green; shellcheck clean; test script prints â€œâœ…Â AllÂ goodâ€.
	â€¢	Cost â‰¤ --cost_ceiling; wallâ€‘time enforced.

â¸»

9Â â€¢ Acceptance Criteria

Running

./build-and-run.sh --repo_url https://github.com/sindresorhus/awesome-nodejs \
  --wall_time 600 --cost_ceiling 10.00 --min_stars 200 \
  --model_planner gpt-4o --model_researcher o3 --model_validator o3

	â€¢	Completes within limits, adds â‰¥Â 1 new link, passes awesomeâ€‘lint.
	â€¢	Subsequent rerun yields no duplicates.
	â€¢	agent.log shows model names, tokens, and cost per call.

â¸»

Begin ImplementationÂ ğŸš€