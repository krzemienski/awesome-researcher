# App Flow Document for the Docker-First “Awesome-List Researcher” Tool

## Onboarding and Sign-In/Sign-Up
A new user begins by cloning the repository or downloading the code and reading the `README.md`. They must have Docker installed on their machine and an OpenAI API key ready. The only required environment variable is `OPENAI_API_KEY`, which the user exports into their shell before running any commands. There is no traditional sign-up or sign-in inside the tool itself, but the user implicitly “signs in” by providing a valid API key. If a user loses or rotates their OpenAI key, they simply export the new key in the same way. With Docker and the API key in place, the user is ready to launch the application.

## Main Dashboard or Home Page
When the user runs the script `./build-and-run.sh` with the required `--repo_url` flag and any optional flags, Docker builds an image and starts the container. The very first console output shows a timestamped message noting the start of the run, the selected repository URL, and the directory where logs and artifacts will appear. This initial screen functions as the home page of the application, giving users a clear view of the run identifier in ISO format, the location of the combined `agent.log`, and the set of flags in use. From this point, the user simply watches the tool’s progress in the same terminal window, while structured logs accumulate in the run directory for later inspection.

## Detailed Feature Flows and Page Transitions
Once Docker spins up the container, the orchestrator in `main.py` reads all the CLI flags and environment variables. It immediately sets a wall-time alarm to enforce the time limit and creates a new run folder using the current ISO 8601 timestamp. The first substantive action is a call to GitHub’s REST API to find the default branch of the target repository. With the branch name in hand, the tool constructs the raw content URL for `README.md` and attempts to download it. If that fails, it retries by accessing `/HEAD/README.md`. A double failure causes the tool to abort with an explicit error message, since it cannot proceed without the original README.

When the raw markdown is retrieved, the parser module reads it and generates `original.json`. The parser enforces the Awesome-List rules by checking for proper headings, alphabetical order, valid HTTPS links, and description length. After parsing, the PlannerAgent takes over by reading `original.json` and using the chosen OpenAI model to expand each category into search queries. It writes out `plan.json`, noting each query and category. Every call to the OpenAI API is checked against the cost ceiling. If the next estimated call would push the total spend over the user’s budget, the planner logs a warning and stops further calls.

With `plan.json` ready, the orchestrator launches one CategoryResearchAgent per listed category. These agents run in parallel inside the container and use SearchTool and BrowserTool to gather candidate resources. Each agent writes its findings to a `candidate_<category>.json` file. If the wall-time is about to expire, the signal alarm causes all agents to finish their current task and shut down gracefully.

After the research agents complete, the aggregator merges all `candidate_*.json` files into a single collection. The duplicate filter then compares that collection against the entries in `original.json` and writes only new, unique links into `new_links.json`. The tool logs the deduplication ratio and ensures no overlap remains. Next comes the Validator module, which performs HTTP HEAD requests to confirm a 200 OK response and checks GitHub star counts against the `--min_stars` threshold. Each valid link’s description is cleaned or trimmed by a final model call if needed.

Finally, the renderer merges `original.json` and `new_links.json` into `updated_list.md`. It runs an awesome-lint fix loop inside the container until the markdown passes all lint rules. The completed `updated_list.md` and lint-compliant artifacts are saved in the run directory, alongside `original.json`, `plan.json`, all candidate files, `new_links.json`, and a human-readable `research_report.md`.

## Settings and Account Management
Users adjust tool behavior through CLI flags such as `--wall_time`, `--cost_ceiling`, `--min_stars`, `--output_dir`, `--seed`, and model overrides for planner, researcher, and validator agents. To change preferences, the user simply re-runs `build-and-run.sh` with different flags in the same environment. The only persistent account credential is the `OPENAI_API_KEY`. There is no interactive settings page; configuration always happens at launch time. After updating any settings, the user returns to the main flow by running the script again, which recreates a fresh run directory and repeats the process with the new parameters.

## Error States and Alternate Paths
If the GitHub API fails to return a default branch name or the raw markdown download does not succeed even after a fallback, the orchestrator aborts with a clear error message and nonzero exit code. During any web request, the tool automatically retries failed calls with exponential back-off to handle transient network issues. When the total cost of pending API calls would exceed the user’s ceiling, the workflow halts further calls and logs a warning, but it still processes existing data through parsing, aggregation, and rendering. If the wall-time limit is reached before research agents finish, the signal handler ensures they stop cleanly and the tool proceeds to aggregation and output. Validation errors such as non-200 responses or links failing the star count check are logged and those links are omitted from the final output. At every step, the user can inspect `agent.log` for ISO timestamps, token usage, and cost to understand why the tool took each action.

## Conclusion and Overall App Journey
From the moment a user exports their OpenAI key and ensures Docker is installed, through the simple step of running a shell script, they experience a fully automated workflow. The tool builds a container, fetches the repository’s README, parses it, generates a plan, runs parallel research agents, deduplicates and validates links, and finally produces a lint-compliant Markdown list. All configuration happens at launch, all operations run inside Docker, and all logs and artifacts end up neatly in a timestamped folder. Typical day-to-day use involves tweaking flags to adjust budgets or models, rerunning the script, and inspecting the updated list and logs. The process is self-contained, predictable, and designed to deliver new, valid resources for any Awesome-style repository with minimal manual intervention.